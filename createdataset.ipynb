{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-18 11:08:37.215675: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-18 11:08:37.588742: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-18 11:08:37.588783: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-18 11:08:37.645639: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-18 11:08:37.769813: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-18 11:08:38.770212: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/himanshu/.local/lib/python3.10/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:15: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from tpot import TPOTClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, GRU, LSTM, Dense, Dropout\n",
    "from keras.initializers import Constant\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import pickle as pkl\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.read_csv(\"FullyLabelled/sentiment-emotion-labelled_Dell_tweets.csv\", index_col=\"Index\")\n",
    "# df.columns=[\"DateTime\", \"TweetID\", \"Text\", \"Username\", \"Sentiment\", \"SScore\", \"Emotion\",\"EScore\"]\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop([\"DateTime\", \"TweetID\", \"Username\"], axis=1, inplace=True)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.Sentiment.unique(), df.Emotion.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.Sentiment.value_counts(), df.Emotion.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdict={'neutral':1, 'positive':0, 'negative':2}\n",
    "edict={'disgust':0,'anger':1, 'sadness':2, 'fear':3,'surprise':4, 'optimism':5, 'joy':6,'anticipation':7}\n",
    "# df.Sentiment=df.Sentiment.replace(sdict)\n",
    "# df.Emotion=df.Emotion.replace(edict)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "# df.Text=df.Text.apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.Text.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text2(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    processed_text = []\n",
    "\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    processed_words = []\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            stemmed_word = stemmer.stem(word)\n",
    "            lemmatized_word = lemmatizer.lemmatize(stemmed_word)\n",
    "            processed_words.append(lemmatized_word)\n",
    "\n",
    "    processed_text.append(' '.join(processed_words))\n",
    "\n",
    "    return processed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.Text=df.Text.apply(process_text2)\n",
    "# df.Text.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets = df['Text'].tolist()\n",
    "# tokenized_tweets = [tweet[0].split() for tweet in tweets]\n",
    "\n",
    "# modelw2v = Word2Vec(tokenized_tweets, vector_size=300, window=5, min_count=10, workers=8)\n",
    "# modelw2v.train(tokenized_tweets, total_examples=len(tokenized_tweets), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelw2v.wv.most_similar(\"ai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X = df.Text\n",
    "# y = df.drop(['Text'], axis=1)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=df[\"Emotion\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test.Emotion.value_counts(), y_train.Emotion.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vec(s, model):\n",
    "    # return np.array([model.wv[word] for word in s[0].split() if word in modelw2v.wv.key_to_index])\n",
    "    vecs = [model.wv[word] for word in s[0].split() if word in model.wv.key_to_index]\n",
    "    return np.mean(vecs, axis=0) if vecs else np.zeros(model.vector_size)\n",
    "\n",
    "\n",
    "# X_train_vec = [sentence_to_vec(s, modelw2v) for s in X_train]\n",
    "# X_test_vec = [sentence_to_vec(s, modelw2v) for s in X_test]\n",
    "\n",
    "# max_len = max(max(len(x) for x in X_train_vec), max(len(x) for x in X_test_vec))\n",
    "\n",
    "# X_train_pad = pad_sequences(X_train_vec, maxlen=max_len, dtype='float32', padding='post')\n",
    "# X_test_pad = pad_sequences(X_test_vec, maxlen=max_len, dtype='float32', padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_pad.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = {\n",
    "#     'Voting Classifier': VotingClassifier(estimators=[\n",
    "#         ('xg', XGBClassifier(use_label_encoder=False, eval_metric='logloss', verbosity=0)),\n",
    "#         ('lgbm', LGBMClassifier(verbose=0, random_state=42)),\n",
    "#         ('hgb', HistGradientBoostingClassifier(random_state=42))\n",
    "#     ], voting='hard'),\n",
    "#     'Logistic Regression': LogisticRegression(verbose=0, max_iter=1000),\n",
    "#     'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "#     'Random Forest': RandomForestClassifier(verbose=0, random_state=42),\n",
    "#     'AdaBoost': AdaBoostClassifier(random_state=42),\n",
    "#     'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', verbosity=0),\n",
    "#     'CatBoost': CatBoostClassifier(verbose=0, random_seed=42),\n",
    "#     'LightGBM': LGBMClassifier(verbose=0, random_state=42),\n",
    "#     'HistGradientBoosting': HistGradientBoostingClassifier(random_state=42)\n",
    "#     # 'TPOT': TPOTClassifier(generations=5, population_size=50, verbosity=0, random_state=42)\n",
    "# }\n",
    "\n",
    "# y_trains = np.array(y_train.Sentiment).reshape(-1,)\n",
    "\n",
    "# for model_name, model in models.items():\n",
    "#     print(f\"Training {model_name}...\")\n",
    "#     model.fit(X_train_pad, y_trains)\n",
    "    \n",
    "#     print(f\"Evaluating {model_name}...\")\n",
    "#     y_pred = model.predict(X_test_pad)\n",
    "#     print(classification_report(y_test.Sentiment, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size = len(modelw2v.wv.key_to_index) + 1\n",
    "# vector_size = modelw2v.wv.vector_size\n",
    "\n",
    "# embedding_matrix = np.zeros((vocab_size, vector_size))\n",
    "# for word, i in modelw2v.wv.key_to_index.items():\n",
    "#     embedding_matrix[i] = modelw2v.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_model(model_type):\n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(vocab_size, vector_size, embeddings_initializer=Constant(embedding_matrix), trainable=True))\n",
    "#     model.add(Dropout(0.5))\n",
    "#     if model_type == 'RNN':\n",
    "#         model.add(SimpleRNN(50))\n",
    "#     elif model_type == 'GRU':\n",
    "#         model.add(GRU(50))\n",
    "#     elif model_type == 'LSTM':\n",
    "#         model.add(LSTM(50))\n",
    "    \n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model_type in ['RNN', 'GRU', 'LSTM']:\n",
    "#     print(f\"Training {model_type} model...\")\n",
    "#     model = create_model(model_type)\n",
    "#     model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#     model.fit(X_train_pad, y_trains, epochs=10, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modele = {\n",
    "#     'Logistic Regression': LogisticRegression(verbose=0, max_iter=1000),\n",
    "#     'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "#     'Random Forest': RandomForestClassifier(verbose=0, random_state=42),\n",
    "#     'AdaBoost': AdaBoostClassifier(random_state=42),\n",
    "#     'Voting Classifier': VotingClassifier(estimators=[\n",
    "#         ('xg', XGBClassifier(use_label_encoder=False, eval_metric='logloss', verbosity=0)),\n",
    "#         ('lgbm', LGBMClassifier(verbose=0, random_state=42)),\n",
    "#         ('hgb', HistGradientBoostingClassifier(random_state=42))\n",
    "#     ], voting='hard'),\n",
    "#     'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', verbosity=0),\n",
    "#     'CatBoost': CatBoostClassifier(verbose=0, random_seed=42),\n",
    "#     'LightGBM': LGBMClassifier(verbose=0, random_state=42),\n",
    "#     'HistGradientBoosting': HistGradientBoostingClassifier(random_state=42)\n",
    "#     # 'TPOT': TPOTClassifier(generations=5, population_size=50, verbosity=0, random_state=42)\n",
    "# }\n",
    "\n",
    "# y_trains = np.array(y_train.Emotion).reshape(-1,)\n",
    "\n",
    "# for model_name, model in modele.items():\n",
    "#     print(f\"Training {model_name}...\")\n",
    "#     model.fit(X_train_pad, y_trains)\n",
    "    \n",
    "#     print(f\"Evaluating {model_name}...\")\n",
    "#     y_pred = model.predict(X_test_pad)\n",
    "#     print(classification_report(y_test.Emotion, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modele = {\n",
    "#     'Linear Regression': LinearRegression(),\n",
    "#     'Decision Tree Regressor': DecisionTreeRegressor(random_state=42),\n",
    "#     'Random Forest Regressor': RandomForestRegressor(random_state=42),\n",
    "#     'AdaBoost Regressor': AdaBoostRegressor(random_state=42),\n",
    "#     'Gradient Boosting Regressor': GradientBoostingRegressor(random_state=42),\n",
    "#     'XGBoost Regressor': XGBRegressor(use_label_encoder=False, eval_metric='logloss', verbosity=0),\n",
    "#     'CatBoost Regressor': CatBoostRegressor(verbose=0, random_seed=42),\n",
    "#     'LightGBM Regressor': LGBMRegressor(random_state=42)\n",
    "# }\n",
    "\n",
    "# y_trains = np.array(y_train.SScore).reshape(-1,)\n",
    "\n",
    "# for model_name, model in modele.items():\n",
    "#     print(f\"Training {model_name}...\")\n",
    "#     model.fit(X_train_pad, y_trains)\n",
    "    \n",
    "#     print(f\"Evaluating {model_name}...\")\n",
    "#     y_pred = model.predict(X_test_pad)\n",
    "#     print('Mean Squared Error:', mean_squared_error(y_test.SScore, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modele = {\n",
    "#     'Linear Regression': LinearRegression(),\n",
    "#     'Decision Tree Regressor': DecisionTreeRegressor(random_state=42),\n",
    "#     'Random Forest Regressor': RandomForestRegressor(random_state=42),\n",
    "#     'AdaBoost Regressor': AdaBoostRegressor(random_state=42),\n",
    "#     'Gradient Boosting Regressor': GradientBoostingRegressor(random_state=42),\n",
    "#     'XGBoost Regressor': XGBRegressor(use_label_encoder=False, eval_metric='logloss', verbosity=0),\n",
    "#     'CatBoost Regressor': CatBoostRegressor(verbose=0, random_seed=42),\n",
    "#     'LightGBM Regressor': LGBMRegressor(random_state=42)\n",
    "# }\n",
    "\n",
    "# y_trains = np.array(y_train.EScore).reshape(-1,)\n",
    "\n",
    "# for model_name, model in modele.items():\n",
    "#     print(f\"Training {model_name}...\")\n",
    "#     model.fit(X_train_pad, y_trains)\n",
    "    \n",
    "#     print(f\"Evaluating {model_name}...\")\n",
    "#     y_pred = model.predict(X_test_pad)\n",
    "#     print('Mean Squared Error:', mean_squared_error(y_test.EScore, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.system(f'cp {\"FullyLabelled/sentiment-emotion-labelled_Dell_tweets.csv\"} {\"TempSmall.csv\"}')\n",
    "# os.system(f'cp {\"SemiLabelled/training.1600000.processed.noemoticon.csv\"} {\"TempLarge.csv\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small=pd.read_csv(\"TempSmall.csv\", index_col=\"Index\")\n",
    "# small.columns=[\"DateTime\", \"TweetID\", \"Text\", \"Username\", \"Sentiment\", \"SScore\", \"Emotion\",\"EScore\"]\n",
    "# small.drop([\"DateTime\", \"TweetID\", \"Username\"], axis=1, inplace=True)\n",
    "# small.Sentiment=small.Sentiment.replace(sdict)\n",
    "# small.Emotion=small.Emotion.replace(edict)\n",
    "# small.Text=small.Text.apply(process_text)\n",
    "# small.Text=small.Text.apply(process_text2)\n",
    "\n",
    "# large=pd.read_csv(\"TempLarge.csv\", names=[\"Sentiment\", \"ID2\", \"DT\", \"Q\", \"UserName\", \"Text\"], encoding=\"ISO-8859-1\")\n",
    "# large.drop([\"DT\", \"ID2\", \"UserName\", \"Q\"], axis=1, inplace=True)\n",
    "# large.Sentiment=large.Sentiment.replace({0:2, 4:0,2:1})\n",
    "# large.Text=large.Text.apply(process_text)\n",
    "# large.Text=large.Text.apply(process_text2)\n",
    "\n",
    "# small.to_csv(\"TempSmall.csv\")\n",
    "# large.to_csv(\"TempLarge.csv\")\n",
    "\n",
    "# tweets = small['Text'].tolist()+large['Text'].tolist()\n",
    "# tokenized_tweets = [tweet[0].split() for tweet in tweets]\n",
    "\n",
    "# modelw2v = Word2Vec(tokenized_tweets, vector_size=300, window=5, min_count=10, workers=8)\n",
    "# modelw2v.train(tokenized_tweets, total_examples=len(tokenized_tweets), epochs=20)\n",
    "\n",
    "# pkl.dump(modelw2v, open(\"W2V_Model.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkl.dump(small, open(\"small.pkl\", \"wb\"))\n",
    "# pkl.dump(large, open(\"large.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "small=pkl.load(open(\"small.pkl\", \"rb\"))\n",
    "# large=pkl.load(open(\"large.pkl\", \"rb\"))\n",
    "large1=pkl.load(open(\"large1.pkl\", \"rb\"))\n",
    "modelw2v=pkl.load(open(\"W2V_Model.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkl.dump(large[:large.shape[0]//2], open(\"large1.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkl.dump(large[large.shape[0]//2:], open(\"large2.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small=pd.read_csv(\"TempSmall.csv\", index_col=\"Index\")\n",
    "# large=pd.read_csv(\"TempLarge.csv\",index_col=\"Index\", encoding=\"ISO-8859-1\",low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_vec = [sentence_to_vec(s, modelw2v) for s in small.Text]\n",
    "X_test_vec = [sentence_to_vec(s, modelw2v) for s in large1.Text]\n",
    "\n",
    "max_len = max(max(len(x) for x in X_train_vec), max(len(x) for x in X_test_vec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_pad = pad_sequences(X_train_vec, maxlen=max_len, dtype='float32', padding='post')\n",
    "X_test_pad = pad_sequences(X_test_vec, maxlen=max_len, dtype='float32', padding='post')\n",
    "\n",
    "y_train=small.drop([\"Text\"], axis=1)\n",
    "y_test=large1.drop([\"Text\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.31407335, -0.07926951,  0.08237156, ...,  0.04243321,\n",
       "         0.07356723,  0.00521673],\n",
       "       [ 0.3245958 , -0.36294833,  0.05073858, ..., -0.0060014 ,\n",
       "         0.6755086 , -0.38814867],\n",
       "       [-0.0997056 , -0.05142836,  0.10781164, ...,  0.29043314,\n",
       "         0.41714585, -0.25970918],\n",
       "       ...,\n",
       "       [ 0.12978235,  0.6730353 ,  0.3436516 , ..., -0.0110016 ,\n",
       "         0.4086631 , -0.6039487 ],\n",
       "       [-0.09105451,  0.1899454 , -0.09428181, ...,  0.24367954,\n",
       "         0.40905598, -0.01783706],\n",
       "       [ 0.33882108, -0.5553169 ,  0.19569983, ...,  0.02169369,\n",
       "         0.5468526 , -0.3540177 ]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# with open('X_train_pad.pkl', 'wb') as f:\n",
    "#     pickle.dump(X_train_pad, f)\n",
    "\n",
    "with open('X_test_pad.pkl', 'wb') as f:\n",
    "    pkl.dump(X_test_pad, f)\n",
    "\n",
    "# # Pickle the y values\n",
    "# with open('y_train.pkl', 'wb') as f:\n",
    "#     pickle.dump(y_train, f)\n",
    "\n",
    "# with open('y_test.pkl', 'wb') as f:\n",
    "#     pickle.dump(y_test, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m     X_train_pad \u001b[38;5;241m=\u001b[39m pkl\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_test_pad.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 5\u001b[0m     X_test_pad \u001b[38;5;241m=\u001b[39m \u001b[43mpkl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "with open('X_train_pad.pkl', 'rb') as f:\n",
    "    X_train_pad = pkl.load(f)\n",
    "\n",
    "with open('X_test_pad.pkl', 'rb') as f:\n",
    "    X_test_pad = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05364796, -0.38689256,  0.09192634, ..., -0.309362  ,\n",
       "         0.35936925, -0.41113326],\n",
       "       [-0.45185354,  0.27520022,  0.3185646 , ..., -0.02325147,\n",
       "         0.40133536, -0.39236057],\n",
       "       [-0.03389205, -0.5166912 ,  0.120496  , ..., -0.2786832 ,\n",
       "         0.09066193, -0.03940602],\n",
       "       ...,\n",
       "       [-0.3303782 , -0.1466112 ,  0.47284228, ...,  0.60390085,\n",
       "         0.08154381, -0.28264546],\n",
       "       [ 0.39953598,  0.26543802,  0.1520131 , ..., -0.40175033,\n",
       "         0.84357166,  0.04084152],\n",
       "       [ 0.31709138,  0.32290527, -0.21794027, ...,  0.82818097,\n",
       "         0.7902313 ,  0.0528754 ]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.31407335, -0.07926951,  0.08237156, ...,  0.04243321,\n",
       "         0.07356723,  0.00521673],\n",
       "       [ 0.3245958 , -0.36294833,  0.05073858, ..., -0.0060014 ,\n",
       "         0.6755086 , -0.38814867],\n",
       "       [-0.0997056 , -0.05142836,  0.10781164, ...,  0.29043314,\n",
       "         0.41714585, -0.25970918],\n",
       "       ...,\n",
       "       [ 0.12978235,  0.6730353 ,  0.3436516 , ..., -0.0110016 ,\n",
       "         0.4086631 , -0.6039487 ],\n",
       "       [-0.09105451,  0.1899454 , -0.09428181, ...,  0.24367954,\n",
       "         0.40905598, -0.01783706],\n",
       "       [ 0.33882108, -0.5553169 ,  0.19569983, ...,  0.02169369,\n",
       "         0.5468526 , -0.3540177 ]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catboost_optimization(cv_splits, X, y, mode=1):\n",
    "    def function(learning_rate, depth, l2_leaf_reg, border_count):\n",
    "        if mode:\n",
    "            return cross_val_score(\n",
    "                CatBoostClassifier(\n",
    "                    learning_rate=learning_rate,\n",
    "                    depth=int(round(depth)),\n",
    "                    l2_leaf_reg=l2_leaf_reg,\n",
    "                    border_count=int(round(border_count)),\n",
    "                    loss_function=\"Logloss\",\n",
    "                    verbose=False,\n",
    "                ),\n",
    "                X=X,\n",
    "                y=y,\n",
    "                cv=cv_splits,\n",
    "                scoring=\"accuracy\",\n",
    "                n_jobs=-1,\n",
    "            ).mean()\n",
    "        else:\n",
    "            return cross_val_score(\n",
    "                CatBoostRegressor(\n",
    "                    learning_rate=learning_rate,\n",
    "                    depth=int(round(depth)),\n",
    "                    l2_leaf_reg=l2_leaf_reg,\n",
    "                    border_count=int(round(border_count)),\n",
    "                    loss_function=\"Logloss\",\n",
    "                    verbose=False,\n",
    "                ),\n",
    "                X=X,\n",
    "                y=y,\n",
    "                cv=cv_splits,\n",
    "                scoring=\"accuracy\",\n",
    "                n_jobs=-1,\n",
    "            ).mean()\n",
    "\n",
    "    return function\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | border... |   depth   | l2_lea... | learni... |\n",
      "-------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "while len(X_test_pad) > 0:\n",
    "    models = {\n",
    "        \"Sentiment\": CatBoostClassifier(verbose=False),\n",
    "        \"Emotion\": CatBoostClassifier(verbose=False),\n",
    "        \"SScore\": CatBoostRegressor(verbose=False),\n",
    "        \"EScore\": CatBoostRegressor(verbose=False),\n",
    "    }\n",
    "\n",
    "    for label in models.keys():\n",
    "        if label==\"SScore\" or label==\"EScore\":\n",
    "            mode=0\n",
    "        else:\n",
    "            mode=1\n",
    "        optimizer = BayesianOptimization(\n",
    "            f=catboost_optimization(5, X_train_pad, y_train[label], mode),\n",
    "            pbounds={\n",
    "                \"learning_rate\": (0.01, 1.0),\n",
    "                \"depth\": (4, 10),\n",
    "                \"l2_leaf_reg\": (1, 10),\n",
    "                \"border_count\": (5, 255)\n",
    "            },\n",
    "            random_state=1234,\n",
    "            verbose=2\n",
    "        )\n",
    "\n",
    "        optimizer.maximize(n_iter=25, init_points=2)\n",
    "        params=optimizer.max[\"params\"]\n",
    "        if mode:\n",
    "            m=CatBoostClassifier(**params)\n",
    "        else:\n",
    "            m=CatBoostRegressor(**params)\n",
    "        \n",
    "        m.fit(X_train_pad, y_train[label])\n",
    "        \n",
    "        models[label]=m\n",
    "\n",
    "    \n",
    "    predictions = {}\n",
    "    for label in models.keys():\n",
    "        predictions[label] = models[label].predict_proba(X_test_pad)\n",
    "\n",
    "    indices = np.where(\n",
    "        (predictions[\"Sentiment\"][:, 1] > 0.9)\n",
    "        & (predictions[\"Emotion\"][:, 1] > 0.9)\n",
    "        & (predictions[\"SScore\"][:, 1] > 0.9)\n",
    "        & (predictions[\"EScore\"][:, 1] > 0.9)\n",
    "    )\n",
    "\n",
    "    X_train_pad = np.concatenate((X_train_pad, X_test_pad[indices]))\n",
    "    y_train[\"Sentiment\"] = np.concatenate(\n",
    "        (y_train[\"Sentiment\"], predictions[\"Sentiment\"][indices, 1])\n",
    "    )\n",
    "    y_train[\"Emotion\"] = np.concatenate(\n",
    "        (y_train[\"Emotion\"], predictions[\"Emotion\"][indices, 1])\n",
    "    )\n",
    "    y_train[\"SScore\"] = np.concatenate(\n",
    "        (y_train[\"SScore\"], predictions[\"SScore\"][indices, 1])\n",
    "    )\n",
    "    y_train[\"EScore\"] = np.concatenate(\n",
    "        (y_train[\"EScore\"], predictions[\"EScore\"][indices, 1])\n",
    "    )\n",
    "\n",
    "    X_test_pad = np.delete(X_test_pad, indices, axis=0)\n",
    "    y_test=np.delete(y_test, indices, axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in models.keys():\n",
    "        models[label].fit(X_train_pad, y_train[label])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
